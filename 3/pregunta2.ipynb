{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; overflow: hidden; text-align:center;\">\n",
    "    <h1>INF-477 Redes Neuronales Artificiales</h1>\n",
    "    <h2>Tarea 3 - Redes Neuronales Recurrentes</h2><br/>\n",
    "    <div style=\"width: 50%; float: left;\">\n",
    "        <h3>Gabriel Jara</h3>\n",
    "        <h4>``gabriel.jara@usm.cl``</h4>\n",
    "        <h4>13550188-3</h4>\n",
    "    </div>\n",
    "    <div style=\"margin-left: 300px;\">\n",
    "        <h3>Daniel San Martín</h3>\n",
    "        <h4>``daniel.sanmartinr@sansano.usm.cl``</h4>\n",
    "        <h4>682016101-5</h4>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de sentimientos usando RNN\n",
    "\n",
    "Las redes recurrentes son utilizadas también en procesamiento de lenguaje natural. En esta ocasión usaremos\n",
    "el *Large Movie Review Dataset*, también conocido como *IMDB dataset* que contiene 50000 comentarios de películas etiquetadas como buenas o malas (50 %-50 % train-testing). El problema consiste en procesar el texto contenido en un comentario para determinar si este es positivo o negativo. Este dataset fue recolectado por investigadores de Stanford [5] y utilizado en la competencia Kaggle [6].\n",
    "\n",
    "<img src=\"img/figure2.png\" />\n",
    "<center>Figura 2: IMDB dataset</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Cargue el dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce 920M (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "np.random.seed(15)\n",
    "srng = RandomStreams(8)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(seed=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Concatene los conjuntos de entrenamiento. ¿Cuántas palabras tiene el dataset? Obtenga un boxplot de la distribución del largo de las palabras. Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg4AAAFkCAYAAABIPLOYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOEiSpGwGB0mSlM3gIEmSsv0/Gc1/RyLi5YoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82580765d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El data set completo tiene 11737946 palabras, distribuidas en 88455 términos diferentes.\n",
      "Las reseñas tienen en promedio 234.75892 palabras, con desviación estándar de 172.911494587.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "print(\"Review length: \")\n",
    "result = map(len, X)\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()\n",
    "\n",
    "print 'El data set completo tiene '+ str(len(list(chain.from_iterable(X))))+ ' palabras, distribuidas en ' +str(max(np.amax(X)))+' términos diferentes.'\n",
    "print 'Las reseñas tienen en promedio '+str(np.mean(result))+' palabras, con desviación estándar de '+str(np.std(result))+'.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Cargue nuevamente el dataset, pero esta vez extrayendo solo las 3000 palabras más relevantes y acotando el largo máximo para un comentario en 500 palabras. Los comentarios con un largo menor deben ser rellenados con 0 para que la red reciba todas los ejemplos del mismo tamaño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "top_words = 3000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words, seed=15)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "pred_x_train = np.zeros(shape=(len(X_train),1))\n",
    "pred_x_test = np.zeros(shape=(len(X_test),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Entrene una red LSTM con una capa de embedding y evalúe su desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que se ha observado que, al menos en la implementación de Keras, las LSTM presentan un alto grado de variabilidad que no logra ser controlada, es que se ha optado por repetir los próximos experimentos en grupos de 5. Para cada configuración se ejecutará 5 veces, luego de lo cual se procederá a obtener la Accuracy media e Intervalo de Confianza de la misma, esto último utilizando test t-student en consideración a lo imitado del tamaño de muestra. Además, para cada modelo constrido se almacenrará las predicciones tanto sobre el training set como sobre el test set, a fin de utilizarlo al final en un stacking de todos los modelos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se declara la función que se utilizará para obtener los intervalos de confianza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0*np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * sp.stats.t.ppf((1+confidence)/2., n-1)\n",
    "    return m, m-h, m+h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se entrena y evalua una red de una capa LSTM de 100 nodos, utilizando función de activación sigmoid, sobre las 3000 palabras más frecuentes vector de embedding de largo 32. Como ya se ha mencionado, se entrenará y evaluará esta configuración en 5 ocasiones, para obtener la exactitud media e intervalo de confianza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 379s - loss: 0.4748 - acc: 0.7690 - val_loss: 0.3946 - val_acc: 0.8241\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 372s - loss: 0.3120 - acc: 0.8723 - val_loss: 0.3099 - val_acc: 0.8696\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 349s - loss: 0.3027 - acc: 0.8796 - val_loss: 0.3240 - val_acc: 0.8639\n",
      "[loss , accuracy] : [0.32396760501146316, 0.86392000000000002]\n",
      " \n",
      "Guardado como: k= 0 - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 362s - loss: 0.5193 - acc: 0.7536 - val_loss: 0.4184 - val_acc: 0.8142\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 363s - loss: 0.3120 - acc: 0.8723 - val_loss: 0.3018 - val_acc: 0.8770\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.2698 - acc: 0.8922 - val_loss: 0.2955 - val_acc: 0.8795\n",
      "[loss , accuracy] : [0.29550797180533411, 0.87951999999999997]\n",
      " \n",
      "Guardado como: k= 1 - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 352s - loss: 0.4671 - acc: 0.7687 - val_loss: 0.3078 - val_acc: 0.8720\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 348s - loss: 0.3444 - acc: 0.8576 - val_loss: 0.3885 - val_acc: 0.8244\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.2854 - acc: 0.8860 - val_loss: 0.3213 - val_acc: 0.8704\n",
      "[loss , accuracy] : [0.3213348641985655, 0.87043999999999999]\n",
      " \n",
      "Guardado como: k= 2 - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.4747 - acc: 0.7670 - val_loss: 0.3784 - val_acc: 0.8326\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.3393 - acc: 0.8628 - val_loss: 0.3213 - val_acc: 0.8658\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 350s - loss: 0.2758 - acc: 0.8899 - val_loss: 0.2937 - val_acc: 0.8788\n",
      "[loss , accuracy] : [0.29365877339720725, 0.87875999999999999]\n",
      " \n",
      "Guardado como: k= 3 - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 357s - loss: 0.5570 - acc: 0.7018 - val_loss: 0.3757 - val_acc: 0.8399\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 357s - loss: 0.3490 - acc: 0.8532 - val_loss: 0.3191 - val_acc: 0.8678\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 357s - loss: 0.3111 - acc: 0.8733 - val_loss: 0.3331 - val_acc: 0.8690\n",
      "[loss , accuracy] : [0.33307223003387454, 0.86895999999999995]\n",
      " \n",
      "Guardado como: k= 4 - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2\n",
      " \n",
      "Acc Media: 0.87232 Intervalo de Confianza: [0.864020853075 , 0.880619146925]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "np.random.seed(15)\n",
    "srng = RandomStreams(8)\n",
    "\n",
    "top_words = 3000\n",
    "embedding_vector_length = 32\n",
    "\n",
    "K = 5\n",
    "score = [None]*K\n",
    "\n",
    "for k in range(0,K):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "    score[k] = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print  \"[loss , accuracy] : \" + str(score[k])\n",
    "    print \" \"\n",
    "    pred_x_train = np.c_[pred_x_train, model.predict(X_train)]\n",
    "    pred_x_test = np.c_[pred_x_test, model.predict(X_test)]\n",
    "    indice = 'k= ' + str(k) + ' - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2' \n",
    "    pred_indice = [indice]\n",
    "    print 'Guardado como: '+str(indice)\n",
    "    print ' '\n",
    "ci = mean_confidence_interval(np.transpose(score)[1])\n",
    "print 'Acc Media: ' + str(ci[0]) + ' Intervalo de Confianza: ['+ str(ci[1])+ ' , '+ str(ci[2])+ ']'\n",
    "print ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aprecia en que la excatitud alcanza en promedio 0.872. Se puede establecer con un 97.5% de confianza, ademas, que con esta configuración otros cinco experimentos obtendrían una media promedio igual o superior a 0.864. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Varíe el tamaño del vector generado por el embedding. Comente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales con nodos LSTM rescatan no solo palabras independientemente, sino conceptos implicitos en el texto, para lo cual aprovechan la técnica de Embedding, consistente en mapear palabras o frases a un vector numérico. Cada observación se reduce entonces a un vector de largo fijo, de acuerdo al mapeo de embedding resultante del entrenamiento. En este caso se probará distintas configuraciones para el largo de dicho vector, siempre bajo la modalidad de 5 experimentos por configuración. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.6422 - acc: 0.6067 - val_loss: 0.4141 - val_acc: 0.8202\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.3820 - acc: 0.8357 - val_loss: 0.3893 - val_acc: 0.8347\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 339s - loss: 0.3514 - acc: 0.8533 - val_loss: 0.4009 - val_acc: 0.8240\n",
      "[loss , accuracy] con vector de 8: [0.40088018536567688, 0.82399999999999995]\n",
      " \n",
      "Guardado como: k= 0 - LSTM100 - top3000 - evl8 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 340s - loss: 0.5184 - acc: 0.7414 - val_loss: 0.3522 - val_acc: 0.8506\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 340s - loss: 0.3215 - acc: 0.8696 - val_loss: 0.4521 - val_acc: 0.7864\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 340s - loss: 0.3481 - acc: 0.8480 - val_loss: 0.3480 - val_acc: 0.8498\n",
      "[loss , accuracy] con vector de 8: [0.34802811271667483, 0.84984000000000004]\n",
      " \n",
      "Guardado como: k= 1 - LSTM100 - top3000 - evl8 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 341s - loss: 0.5523 - acc: 0.7097 - val_loss: 0.4383 - val_acc: 0.7944\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 343s - loss: 0.3946 - acc: 0.8342 - val_loss: 0.3339 - val_acc: 0.8600\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 341s - loss: 0.2917 - acc: 0.8826 - val_loss: 0.3116 - val_acc: 0.8707\n",
      "[loss , accuracy] con vector de 8: [0.31160893960952757, 0.87072000000000005]\n",
      " \n",
      "Guardado como: k= 2 - LSTM100 - top3000 - evl8 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 340s - loss: 0.6351 - acc: 0.6129 - val_loss: 0.4219 - val_acc: 0.8056\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 340s - loss: 0.4018 - acc: 0.8199 - val_loss: 0.3834 - val_acc: 0.8315\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 341s - loss: 0.3447 - acc: 0.8526 - val_loss: 0.3531 - val_acc: 0.8498\n",
      "[loss , accuracy] con vector de 8: [0.35311285054445268, 0.84975999999999996]\n",
      " \n",
      "Guardado como: k= 3 - LSTM100 - top3000 - evl8 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 340s - loss: 0.5174 - acc: 0.7341 - val_loss: 0.4067 - val_acc: 0.8264\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 340s - loss: 0.3357 - acc: 0.8594 - val_loss: 0.3537 - val_acc: 0.8484\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 339s - loss: 0.2980 - acc: 0.8800 - val_loss: 0.3118 - val_acc: 0.8740\n",
      "[loss , accuracy] con vector de 8: [0.31182265286922456, 0.874]\n",
      " \n",
      "Guardado como: k= 4 - LSTM100 - top3000 - evl8 - sigmoid - binary - adam\n",
      " \n",
      "Acc Media: 0.853664 Intervalo de Confianza: [0.828720169941 , 0.878607830059]\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 348s - loss: 0.4952 - acc: 0.7552 - val_loss: 0.3562 - val_acc: 0.8507\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 348s - loss: 0.3062 - acc: 0.8759 - val_loss: 0.3455 - val_acc: 0.8492\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.2765 - acc: 0.8884 - val_loss: 0.3056 - val_acc: 0.8722\n",
      "[loss , accuracy] con vector de 32: [0.30556820195436479, 0.87216000000000005]\n",
      " \n",
      "Guardado como: k= 0 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.5508 - acc: 0.7110 - val_loss: 0.3762 - val_acc: 0.8388\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 348s - loss: 0.3826 - acc: 0.8324 - val_loss: 0.3460 - val_acc: 0.8556\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 359s - loss: 0.3249 - acc: 0.8678 - val_loss: 0.3303 - val_acc: 0.8619\n",
      "[loss , accuracy] con vector de 32: [0.33028985642433167, 0.86187999999999998]\n",
      " \n",
      "Guardado como: k= 1 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 357s - loss: 0.5056 - acc: 0.7541 - val_loss: 0.3890 - val_acc: 0.8445\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 357s - loss: 0.3091 - acc: 0.8757 - val_loss: 0.3437 - val_acc: 0.8561\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 357s - loss: 0.2674 - acc: 0.8942 - val_loss: 0.3201 - val_acc: 0.8748\n",
      "[loss , accuracy] con vector de 32: [0.32006746590614321, 0.87475999999999998]\n",
      " \n",
      "Guardado como: k= 2 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 355s - loss: 0.4860 - acc: 0.7633 - val_loss: 0.4604 - val_acc: 0.7912\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 355s - loss: 0.3577 - acc: 0.8506 - val_loss: 0.3237 - val_acc: 0.8653\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 355s - loss: 0.3093 - acc: 0.8744 - val_loss: 0.3093 - val_acc: 0.8732\n",
      "[loss , accuracy] con vector de 32: [0.30932173775911331, 0.87316000000000005]\n",
      " \n",
      "Guardado como: k= 3 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 355s - loss: 0.4466 - acc: 0.7916 - val_loss: 0.3189 - val_acc: 0.8670\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 354s - loss: 0.3504 - acc: 0.8520 - val_loss: 0.3363 - val_acc: 0.8596\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 354s - loss: 0.2995 - acc: 0.8783 - val_loss: 0.3285 - val_acc: 0.8569\n",
      "[loss , accuracy] con vector de 32: [0.3285394473409653, 0.85687999999999998]\n",
      " \n",
      "Guardado como: k= 4 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Acc Media: 0.867768 Intervalo de Confianza: [0.857942591096 , 0.877593408904]\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 364s - loss: 0.5429 - acc: 0.7167 - val_loss: 0.3638 - val_acc: 0.8451\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 364s - loss: 0.3214 - acc: 0.8688 - val_loss: 0.3123 - val_acc: 0.8696\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 365s - loss: 0.2690 - acc: 0.8922 - val_loss: 0.3124 - val_acc: 0.8631\n",
      "[loss , accuracy] con vector de 64: [0.3124231669187546, 0.86312]\n",
      " \n",
      "Guardado como: k= 0 - LSTM100 - top3000 - evl64 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 365s - loss: 0.4553 - acc: 0.7870 - val_loss: 0.3416 - val_acc: 0.8606\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 365s - loss: 0.3036 - acc: 0.8758 - val_loss: 0.3383 - val_acc: 0.8615\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 365s - loss: 0.2655 - acc: 0.8949 - val_loss: 0.2995 - val_acc: 0.8763\n",
      "[loss , accuracy] con vector de 64: [0.29951711988925933, 0.87631999999999999]\n",
      " \n",
      "Guardado como: k= 1 - LSTM100 - top3000 - evl64 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 366s - loss: 0.4766 - acc: 0.7722 - val_loss: 0.3537 - val_acc: 0.8506\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 366s - loss: 0.3079 - acc: 0.8767 - val_loss: 0.2994 - val_acc: 0.8762\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 366s - loss: 0.2734 - acc: 0.8895 - val_loss: 0.3622 - val_acc: 0.8398\n",
      "[loss , accuracy] con vector de 64: [0.36220579710245132, 0.83975999999999995]\n",
      " \n",
      "Guardado como: k= 2 - LSTM100 - top3000 - evl64 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 366s - loss: 0.4901 - acc: 0.7546 - val_loss: 0.3755 - val_acc: 0.8441\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 366s - loss: 0.3524 - acc: 0.8532 - val_loss: 0.3292 - val_acc: 0.8641\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 365s - loss: 0.2767 - acc: 0.8890 - val_loss: 0.3657 - val_acc: 0.8454\n",
      "[loss , accuracy] con vector de 64: [0.36567200289249419, 0.84540000000000004]\n",
      " \n",
      "Guardado como: k= 3 - LSTM100 - top3000 - evl64 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 366s - loss: 0.5050 - acc: 0.7630 - val_loss: 0.3594 - val_acc: 0.8488\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 367s - loss: 0.3608 - acc: 0.8478 - val_loss: 0.4305 - val_acc: 0.8301\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 366s - loss: 0.3013 - acc: 0.8787 - val_loss: 0.3201 - val_acc: 0.8672\n",
      "[loss , accuracy] con vector de 64: [0.32012354006052018, 0.86716000000000004]\n",
      " \n",
      "Guardado como: k= 4 - LSTM100 - top3000 - evl64 - sigmoid - binary - adam\n",
      " \n",
      "Acc Media: 0.858352 Intervalo de Confianza: [0.839352304341 , 0.877351695659]\n",
      " \n",
      "Embedding: 8 => Accuracy media:0.853664 , mayor que: 0.828720169941\n",
      " \n",
      "Embedding: 32 => Accuracy media:0.867768 , mayor que: 0.857942591096\n",
      " \n",
      "Embedding: 64 => Accuracy media:0.858352 , mayor que: 0.839352304341\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "np.random.seed(15)\n",
    "srng = RandomStreams(8)\n",
    "\n",
    "top_words = 3000\n",
    "embedding_vector_length = [8, 32, 64]\n",
    "\n",
    "K = 5\n",
    "scores_lb = [None]*len(embedding_vector_length)\n",
    "scores_me = [None]*len(embedding_vector_length)\n",
    "score = [None]*K\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(embedding_vector_length)):\n",
    "    for k in range(0,K):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(top_words, embedding_vector_length[i], input_length=500))\n",
    "        model.add(LSTM(100))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "        score[k] = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print \"[loss , accuracy] con vector de \" + str(embedding_vector_length[i]) + \": \" + str(score[k])\n",
    "        print \" \"\n",
    "        pred_x_train = np.c_[pred_x_train, model.predict(X_train)]\n",
    "        pred_x_test = np.c_[pred_x_test, model.predict(X_test)]\n",
    "        indice = 'k= ' + str(k) + ' - LSTM100 - top3000 - evl' + str(embedding_vector_length[i]) + ' - sigmoid - binary - adam'\n",
    "        pred_indice = [pred_indice , indice]\n",
    "        print 'Guardado como: '+str(indice)\n",
    "        print ' '\n",
    "    ci = mean_confidence_interval(np.transpose(score)[1])\n",
    "    print 'Acc Media: ' + str(ci[0]) + ' Intervalo de Confianza: ['+ str(ci[1])+ ' , '+ str(ci[2])+ ']'\n",
    "    print ' '\n",
    "    scores_me[i] = ci[0]\n",
    "    scores_lb[i] = ci[1]\n",
    "for i in range(0,len(embedding_vector_length)):\n",
    "    print 'Embedding: '+ str(embedding_vector_length[i]) + ' => Accuracy media:'+ str(scores_me[i]) +' , mayor que: ' + str(scores_lb[i])\n",
    "    print ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182370392b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "emb = np.array([8, 32, 64])\n",
    "acc = np.array([0.853664, 0.867768, 0.858352])\n",
    "\n",
    "plt.plot(emb, acc)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Embedding\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado del experimento indicaría que aumentar el embedding en princio mejora la capacidad del modelo, pero existe un tamaño ideal por sobre el cual la estimación en el test set empeora, probablemente debido a sobreajuste. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Repita el proceso cambiando el número de palabras top seleccionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación lo mismo, pero variando el número de palabras más frecuentes que se tomará en cuenta en el analisis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.5199 - acc: 0.7302 - val_loss: 0.4144 - val_acc: 0.8142\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 346s - loss: 0.3627 - acc: 0.8466 - val_loss: 0.3387 - val_acc: 0.8571\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.3394 - acc: 0.8587 - val_loss: 0.3617 - val_acc: 0.8478\n",
      "[loss , accuracy] con 1000 palabras: [0.36169387331008912, 0.84784000000000004]\n",
      " \n",
      "Guardado como: k= 0 - LSTM100 - top1000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 345s - loss: 0.5142 - acc: 0.7396 - val_loss: 0.3896 - val_acc: 0.8314\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 350s - loss: 0.5055 - acc: 0.7566 - val_loss: 0.4534 - val_acc: 0.7910\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.5657 - acc: 0.7046 - val_loss: 0.5394 - val_acc: 0.7256\n",
      "[loss , accuracy] con 1000 palabras: [0.5394100896167755, 0.72555999999999998]\n",
      " \n",
      "Guardado como: k= 1 - LSTM100 - top1000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.4954 - acc: 0.7504 - val_loss: 0.3578 - val_acc: 0.8490\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 350s - loss: 0.3636 - acc: 0.8467 - val_loss: 0.3931 - val_acc: 0.8398\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.3278 - acc: 0.8640 - val_loss: 0.3594 - val_acc: 0.8582\n",
      "[loss , accuracy] con 1000 palabras: [0.35938100358009339, 0.85819999999999996]\n",
      " \n",
      "Guardado como: k= 2 - LSTM100 - top1000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 348s - loss: 0.6143 - acc: 0.6722 - val_loss: 0.6066 - val_acc: 0.6911\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 346s - loss: 0.4953 - acc: 0.7656 - val_loss: 0.6208 - val_acc: 0.6627\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 353s - loss: 0.5007 - acc: 0.7614 - val_loss: 0.4239 - val_acc: 0.8066\n",
      "[loss , accuracy] con 1000 palabras: [0.42393226978778842, 0.80659999999999998]\n",
      " \n",
      "Guardado como: k= 3 - LSTM100 - top1000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 358s - loss: 0.5114 - acc: 0.7480 - val_loss: 0.3964 - val_acc: 0.8253\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 358s - loss: 0.3876 - acc: 0.8310 - val_loss: 0.4192 - val_acc: 0.8110\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 362s - loss: 0.3734 - acc: 0.8412 - val_loss: 0.3388 - val_acc: 0.8582\n",
      "[loss , accuracy] con 1000 palabras: [0.33876494938373564, 0.85816000000000003]\n",
      " \n",
      "Guardado como: k= 4 - LSTM100 - top1000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Acc Media: 0.819272 Intervalo de Confianza: [0.749070435474 , 0.889473564526]\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 359s - loss: 0.5084 - acc: 0.7578 - val_loss: 0.3516 - val_acc: 0.8531\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 360s - loss: 0.3181 - acc: 0.8699 - val_loss: 0.3271 - val_acc: 0.8581\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 360s - loss: 0.2753 - acc: 0.8928 - val_loss: 0.4182 - val_acc: 0.8139\n",
      "[loss , accuracy] con 3000 palabras: [0.41822528602600095, 0.81388000000000005]\n",
      " \n",
      "Guardado como: k= 0 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 358s - loss: 0.5038 - acc: 0.7532 - val_loss: 0.3857 - val_acc: 0.8345\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 348s - loss: 0.3194 - acc: 0.8680 - val_loss: 0.3322 - val_acc: 0.8604\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 348s - loss: 0.2864 - acc: 0.8831 - val_loss: 0.2989 - val_acc: 0.8780\n",
      "[loss , accuracy] con 3000 palabras: [0.29888404138565061, 0.87795999999999996]\n",
      " \n",
      "Guardado como: k= 1 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 348s - loss: 0.5859 - acc: 0.6789 - val_loss: 0.3862 - val_acc: 0.8380\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 348s - loss: 0.3418 - acc: 0.8566 - val_loss: 0.3016 - val_acc: 0.8740\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 348s - loss: 0.2749 - acc: 0.8910 - val_loss: 0.3704 - val_acc: 0.8538\n",
      "[loss , accuracy] con 3000 palabras: [0.37043018138885497, 0.8538]\n",
      " \n",
      "Guardado como: k= 2 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 351s - loss: 0.5321 - acc: 0.7284 - val_loss: 0.3574 - val_acc: 0.8473\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 346s - loss: 0.3169 - acc: 0.8706 - val_loss: 0.2966 - val_acc: 0.8787\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.2654 - acc: 0.8952 - val_loss: 0.3146 - val_acc: 0.8674\n",
      "[loss , accuracy] con 3000 palabras: [0.31457340992450716, 0.86739999999999995]\n",
      " \n",
      "Guardado como: k= 3 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.4725 - acc: 0.7765 - val_loss: 0.3844 - val_acc: 0.8351\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.3013 - acc: 0.8784 - val_loss: 0.3955 - val_acc: 0.8253\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.2790 - acc: 0.8896 - val_loss: 0.3005 - val_acc: 0.8764\n",
      "[loss , accuracy] con 3000 palabras: [0.3004963474404812, 0.87636000000000003]\n",
      " \n",
      "Guardado como: k= 4 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Acc Media: 0.85788 Intervalo de Confianza: [0.825099167431 , 0.890660832569]\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 351s - loss: 0.6076 - acc: 0.6596 - val_loss: 0.4477 - val_acc: 0.8022\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 348s - loss: 0.3274 - acc: 0.8659 - val_loss: 0.4264 - val_acc: 0.7976\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 349s - loss: 0.2614 - acc: 0.8965 - val_loss: 0.3481 - val_acc: 0.8506\n",
      "[loss , accuracy] con 6000 palabras: [0.34813699563503264, 0.85060000000000002]\n",
      " \n",
      "Guardado como: k= 0 - LSTM100 - top6000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 346s - loss: 0.5219 - acc: 0.7469 - val_loss: 0.3252 - val_acc: 0.8637\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 355s - loss: 0.3052 - acc: 0.8784 - val_loss: 0.3282 - val_acc: 0.8640\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 345s - loss: 0.2540 - acc: 0.9029 - val_loss: 0.3189 - val_acc: 0.8694\n",
      "[loss , accuracy] con 6000 palabras: [0.31890318858861921, 0.86943999999999999]\n",
      " \n",
      "Guardado como: k= 1 - LSTM100 - top6000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.4818 - acc: 0.7607 - val_loss: 0.3742 - val_acc: 0.8447\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 346s - loss: 0.3088 - acc: 0.8784 - val_loss: 0.3656 - val_acc: 0.8429\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 354s - loss: 0.2425 - acc: 0.9070 - val_loss: 0.3010 - val_acc: 0.8784\n",
      "[loss , accuracy] con 6000 palabras: [0.30099024756670001, 0.87844]\n",
      " \n",
      "Guardado como: k= 2 - LSTM100 - top6000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 358s - loss: 0.4827 - acc: 0.7662 - val_loss: 0.3593 - val_acc: 0.8491\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 358s - loss: 0.2898 - acc: 0.8854 - val_loss: 0.3306 - val_acc: 0.8592\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 346s - loss: 0.2472 - acc: 0.9028 - val_loss: 0.3018 - val_acc: 0.8788\n",
      "[loss , accuracy] con 6000 palabras: [0.30177505642533303, 0.87880000000000003]\n",
      " \n",
      "Guardado como: k= 3 - LSTM100 - top6000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.4866 - acc: 0.7625 - val_loss: 0.3058 - val_acc: 0.8744\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.2791 - acc: 0.8897 - val_loss: 0.3113 - val_acc: 0.8716\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 355s - loss: 0.2255 - acc: 0.9127 - val_loss: 0.3130 - val_acc: 0.8692\n",
      "[loss , accuracy] con 6000 palabras: [0.31298464787840841, 0.86924000000000001]\n",
      " \n",
      "Guardado como: k= 4 - LSTM100 - top6000 - evl32 - sigmoid - binary - adam\n",
      " \n",
      "Acc Media: 0.869304 Intervalo de Confianza: [0.855099250982 , 0.883508749018]\n",
      " \n",
      "Top Words: 1000 => Accuracy media:0.819272 , mayor que: 0.749070435474\n",
      " \n",
      "Top Words: 3000 => Accuracy media:0.85788 , mayor que: 0.825099167431\n",
      " \n",
      "Top Words: 6000 => Accuracy media:0.869304 , mayor que: 0.855099250982\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "np.random.seed(15)\n",
    "srng = RandomStreams(8)\n",
    "\n",
    "top_words = [1000, 3000, 6000]\n",
    "embedding_vector_length = 32\n",
    "\n",
    "K = 5\n",
    "scores_lb = [None]*len(top_words)\n",
    "scores_me = [None]*len(top_words)\n",
    "score = [None]*K\n",
    "\n",
    "for i in range(0,len(top_words)):\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words[i], seed=15)\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "    for k in range(0,K):    \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(top_words[i], embedding_vector_length, input_length=500))\n",
    "        model.add(LSTM(100))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "        score[k] = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print  \"[loss , accuracy] con \" + str(top_words[i]) + \" palabras: \" + str(score[k])\n",
    "        print \" \"\n",
    "        pred_x_train = np.c_[pred_x_train, model.predict(X_train)]\n",
    "        pred_x_test = np.c_[pred_x_test, model.predict(X_test)]\n",
    "        indice = 'k= ' + str(k) + ' - LSTM100 - top' + str(top_words[i]) +' - evl32 - sigmoid - binary - adam'\n",
    "        pred_indice = [pred_indice , indice]\n",
    "        print 'Guardado como: '+str(indice)\n",
    "        print ' '\n",
    "    ci = mean_confidence_interval(np.transpose(score)[1])\n",
    "    print 'Acc Media: ' + str(ci[0]) + ' Intervalo de Confianza: ['+ str(ci[1])+ ' , '+ str(ci[2])+ ']'\n",
    "    print ' '\n",
    "    scores_me[i] = ci[0]\n",
    "    scores_lb[i] = ci[1]\n",
    "for i in range(0,len(top_words)):\n",
    "    print 'Top Words: '+ str(top_words[i]) + ' => Accuracy media:'+ str(scores_me[i]) +' , mayor que: ' + str(scores_lb[i])\n",
    "    print ' '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqZiIiEir/S+KULzBUHJbyQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182368f7d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "words = np.array([1000, 3000, 6000])\n",
    "acc = np.array([0.819272, 0.85788, 0.869304])\n",
    "\n",
    "plt.plot(words, acc)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Top Words\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El experimento realizado indica que aumentar el número de palabras más frecuentes activas para efectos de análisis, ayuda a mejorar los resultados. Al menos en el límite de palabras seleccionadas en este caso, que fueron 6000, no hay aún señales de sobreajuste del modelo.\n",
    "\n",
    "Pese a lo anterior, con fin de mantener la comparación, los siguientes experimentos se mantendrán con la configuración inicial de 3000 palabras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) Use dropout para entrenar la LSTM. ¿El dropout mejora el desempeño de la red? Señale cuales podrían ser las causas del comportamiento observado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se probará la tecnica de regularización denominada Dropout, que consiste en desactivar con alguna probabilidad cada nodo, durante el proceso de entrenamiento, forzando a la red a encontrar soluciones más generales. Se probará distintas configuraciones de probabilidad de activar cada nodos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 353s - loss: 0.5331 - acc: 0.7232 - val_loss: 0.3606 - val_acc: 0.8508\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 350s - loss: 0.3214 - acc: 0.8695 - val_loss: 0.3103 - val_acc: 0.8730\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 349s - loss: 0.2664 - acc: 0.8931 - val_loss: 0.3163 - val_acc: 0.8726\n",
      "[loss , accuracy] con Dropout: 0.1: [0.31628254446268084, 0.87263999999999997]\n",
      " \n",
      "Guardado como: k= 0 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.1\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 348s - loss: 0.4955 - acc: 0.7531 - val_loss: 0.3728 - val_acc: 0.8515\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 347s - loss: 0.3258 - acc: 0.8650 - val_loss: 0.3337 - val_acc: 0.8598\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 348s - loss: 0.2892 - acc: 0.8858 - val_loss: 0.3256 - val_acc: 0.8612\n",
      "[loss , accuracy] con Dropout: 0.1: [0.3255580536556244, 0.86124000000000001]\n",
      " \n",
      "Guardado como: k= 1 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.1\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 349s - loss: 0.5368 - acc: 0.7265 - val_loss: 0.3631 - val_acc: 0.8438\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 358s - loss: 0.3583 - acc: 0.8505 - val_loss: 0.4490 - val_acc: 0.7784\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 365s - loss: 0.2869 - acc: 0.8867 - val_loss: 0.3320 - val_acc: 0.8691\n",
      "[loss , accuracy] con Dropout: 0.1: [0.33198903693199155, 0.86912]\n",
      " \n",
      "Guardado como: k= 2 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.1\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 365s - loss: 0.4560 - acc: 0.7817 - val_loss: 0.3463 - val_acc: 0.8609\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 365s - loss: 0.3036 - acc: 0.8770 - val_loss: 0.3065 - val_acc: 0.8707\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 365s - loss: 0.2848 - acc: 0.8874 - val_loss: 0.3145 - val_acc: 0.8644\n",
      "[loss , accuracy] con Dropout: 0.1: [0.31449887198209764, 0.86443999999999999]\n",
      " \n",
      "Guardado como: k= 3 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.1\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 530s - loss: 0.4911 - acc: 0.7594 - val_loss: 1.5798 - val_acc: 0.5291\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 531s - loss: 0.7094 - acc: 0.6308 - val_loss: 0.5809 - val_acc: 0.6968\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 530s - loss: 0.3989 - acc: 0.8276 - val_loss: 0.3669 - val_acc: 0.8476\n",
      "[loss , accuracy] con Dropout: 0.1: [0.36691634894847869, 0.84755999999999998]\n",
      " \n",
      "Guardado como: k= 4 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.1\n",
      " \n",
      "Acc Media: 0.863 Intervalo de Confianza: [0.850994502399 , 0.875005497601]\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 528s - loss: 0.5542 - acc: 0.6967 - val_loss: 0.3592 - val_acc: 0.8458\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 528s - loss: 0.3291 - acc: 0.8634 - val_loss: 0.3053 - val_acc: 0.8738\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 528s - loss: 0.3006 - acc: 0.8779 - val_loss: 0.2920 - val_acc: 0.8782\n",
      "[loss , accuracy] con Dropout: 0.2: [0.29198778372406958, 0.87816000000000005]\n",
      " \n",
      "Guardado como: k= 0 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 535s - loss: 0.5038 - acc: 0.7514 - val_loss: 0.3457 - val_acc: 0.8551\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 534s - loss: 0.3662 - acc: 0.8462 - val_loss: 0.3457 - val_acc: 0.8553\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 533s - loss: 0.3215 - acc: 0.8704 - val_loss: 0.3325 - val_acc: 0.8621\n",
      "[loss , accuracy] con Dropout: 0.2: [0.33253656729698183, 0.86207999999999996]\n",
      " \n",
      "Guardado como: k= 1 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 534s - loss: 0.4915 - acc: 0.7546 - val_loss: 0.4305 - val_acc: 0.8180\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 533s - loss: 0.3260 - acc: 0.8637 - val_loss: 0.3122 - val_acc: 0.8697\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 535s - loss: 0.2793 - acc: 0.8880 - val_loss: 0.3117 - val_acc: 0.8742\n",
      "[loss , accuracy] con Dropout: 0.2: [0.31168419624567034, 0.87416000000000005]\n",
      " \n",
      "Guardado como: k= 2 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 533s - loss: 0.5647 - acc: 0.7015 - val_loss: 0.3801 - val_acc: 0.8373\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 533s - loss: 0.3531 - acc: 0.8528 - val_loss: 0.3508 - val_acc: 0.8512\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 533s - loss: 0.4449 - acc: 0.7922 - val_loss: 0.6686 - val_acc: 0.5524\n",
      "[loss , accuracy] con Dropout: 0.2: [0.66859068141937261, 0.5524]\n",
      " \n",
      "Guardado como: k= 3 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 532s - loss: 0.4871 - acc: 0.7594 - val_loss: 0.3626 - val_acc: 0.8493\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 532s - loss: 0.3254 - acc: 0.8670 - val_loss: 0.3271 - val_acc: 0.8628\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 532s - loss: 0.2786 - acc: 0.8879 - val_loss: 0.3333 - val_acc: 0.8731\n",
      "[loss , accuracy] con Dropout: 0.2: [0.33326730901420115, 0.87307999999999997]\n",
      " \n",
      "Guardado como: k= 4 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.2\n",
      " \n",
      "Acc Media: 0.807976 Intervalo de Confianza: [0.630423495685 , 0.985528504315]\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 535s - loss: 0.5669 - acc: 0.6948 - val_loss: 0.3671 - val_acc: 0.8387\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 535s - loss: 0.3595 - acc: 0.8506 - val_loss: 0.3283 - val_acc: 0.8658\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 534s - loss: 0.3133 - acc: 0.8719 - val_loss: 0.3263 - val_acc: 0.8708\n",
      "[loss , accuracy] con Dropout: 0.5: [0.32629164655685428, 0.87083999999999995]\n",
      " \n",
      "Guardado como: k= 0 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.5\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 531s - loss: 0.5294 - acc: 0.7277 - val_loss: 0.4008 - val_acc: 0.8279\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 531s - loss: 0.3651 - acc: 0.8486 - val_loss: 0.4767 - val_acc: 0.8180\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 530s - loss: 0.3202 - acc: 0.8713 - val_loss: 0.3332 - val_acc: 0.8610\n",
      "[loss , accuracy] con Dropout: 0.5: [0.33317296365737914, 0.86095999999999995]\n",
      " \n",
      "Guardado como: k= 1 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.5\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 537s - loss: 0.5334 - acc: 0.7295 - val_loss: 0.3616 - val_acc: 0.8470\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 535s - loss: 0.3273 - acc: 0.8666 - val_loss: 0.3081 - val_acc: 0.8761\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 536s - loss: 0.2877 - acc: 0.8846 - val_loss: 0.3163 - val_acc: 0.8743\n",
      "[loss , accuracy] con Dropout: 0.5: [0.31625313364744184, 0.87427999999999995]\n",
      " \n",
      "Guardado como: k= 2 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.5\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 536s - loss: 0.5674 - acc: 0.6924 - val_loss: 0.4056 - val_acc: 0.8198\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 535s - loss: 0.3622 - acc: 0.8505 - val_loss: 0.3063 - val_acc: 0.8718\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 535s - loss: 0.3231 - acc: 0.8680 - val_loss: 0.3678 - val_acc: 0.8332\n",
      "[loss , accuracy] con Dropout: 0.5: [0.36776690041542054, 0.83316000000000001]\n",
      " \n",
      "Guardado como: k= 3 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.5\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 495s - loss: 0.5805 - acc: 0.6792 - val_loss: 0.3576 - val_acc: 0.8472\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 349s - loss: 0.3329 - acc: 0.8631 - val_loss: 0.3107 - val_acc: 0.8711\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 349s - loss: 0.2984 - acc: 0.8815 - val_loss: 0.3177 - val_acc: 0.8781\n",
      "[loss , accuracy] con Dropout: 0.5: [0.31771909922599795, 0.87812000000000001]\n",
      " \n",
      "Guardado como: k= 4 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.5\n",
      " \n",
      "Acc Media: 0.863472 Intervalo de Confianza: [0.840994866169 , 0.885949133831]\n",
      " \n",
      "Dropout: 0.1 => Accuracy media:0.863 , mayor que: 0.850994502399\n",
      " \n",
      "Dropout: 0.2 => Accuracy media:0.807976 , mayor que: 0.630423495685\n",
      " \n",
      "Dropout: 0.5 => Accuracy media:0.863472 , mayor que: 0.840994866169\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dropout\n",
    "\n",
    "np.random.seed(15)\n",
    "srng = RandomStreams(8)\n",
    "\n",
    "top_words = 3000\n",
    "embedding_vector_length = 32\n",
    "dropout = [0.1, 0.2, 0.5]\n",
    "\n",
    "K = 5\n",
    "scores_lb = [None]*len(dropout)\n",
    "scores_me = [None]*len(dropout)\n",
    "score = [None]*K\n",
    "\n",
    "for i in range(0,len(dropout)):\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words, seed=15)\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "    for k in range(0,K):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "        model.add(Dropout(dropout[i]))\n",
    "        model.add(LSTM(100))\n",
    "        model.add(Dropout(dropout[i]))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "        score[k] = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print  \"[loss , accuracy] con Dropout: \" + str(dropout[i]) + \": \" + str(score[k])\n",
    "        print \" \"\n",
    "        pred_x_train = np.c_[pred_x_train, model.predict(X_train)]\n",
    "        pred_x_test = np.c_[pred_x_test, model.predict(X_test)]\n",
    "        indice = 'k= ' + str(k) + ' - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout' + str(dropout[i])\n",
    "        pred_indice = [pred_indice , indice]\n",
    "        print 'Guardado como: '+str(indice)\n",
    "        print ' '\n",
    "    ci = mean_confidence_interval(np.transpose(score)[1])\n",
    "    print 'Acc Media: ' + str(ci[0]) + ' Intervalo de Confianza: ['+ str(ci[1])+ ' , '+ str(ci[2])+ ']'\n",
    "    print ' '\n",
    "    scores_me[i] = ci[0]\n",
    "    scores_lb[i] = ci[1]\n",
    "for i in range(0,len(dropout)):\n",
    "    print 'Dropout: '+ str(dropout[i]) + ' => Accuracy media:'+ str(scores_me[i]) +' , mayor que: ' + str(scores_lb[i])\n",
    "    print ' '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEPCAYAAACDTflkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1823793fe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "drop = np.array([0.1, 0.2, 0.5])\n",
    "acc = np.array([0.863, 0.807976, 0.863472])\n",
    "\n",
    "plt.plot(drop, acc)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Dropout\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados con Dropout resultaron peores que sin esta técnica, y además se observa cierta inestabilidad, puesto que uno de los 15 experimentos realizados obtuvo una precisión mucho más baja que el resto. Esto se puede deber a que, en la configuración original del experimento, no se presenta sobreajuste. \n",
    "\n",
    "En los siguientes experimentos, donde se profundiza la red, se mantendrá un Dropout moderado como medida de mitigación de eventual sobreajuste. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(h) Proponga un modelo que rebaje el error de test respecto de los modelos estudiados anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se probará en primer lugar modificar la función de activación por ReLu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 350s - loss: 0.6296 - acc: 0.6616 - val_loss: 0.4405 - val_acc: 0.8141\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 350s - loss: 0.5368 - acc: 0.7606 - val_loss: 0.4375 - val_acc: 0.8243\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 350s - loss: 0.4462 - acc: 0.8164 - val_loss: 0.4361 - val_acc: 0.8435\n",
      "[loss , accuracy] con ReLu: [0.43611673567771914, 0.84348000000000001]\n",
      " \n",
      "Guardado como: k= 0 - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 361s - loss: 0.6349 - acc: 0.6764 - val_loss: 0.4664 - val_acc: 0.7930\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 365s - loss: 0.5103 - acc: 0.7771 - val_loss: 0.4965 - val_acc: 0.7897\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 386s - loss: 0.5330 - acc: 0.7549 - val_loss: 0.5087 - val_acc: 0.7808\n",
      "[loss , accuracy] con ReLu: [0.50873766393661501, 0.78080000000000005]\n",
      " \n",
      "Guardado como: k= 1 - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 352s - loss: 0.6499 - acc: 0.6557 - val_loss: 0.5259 - val_acc: 0.7709\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 353s - loss: 0.5602 - acc: 0.7484 - val_loss: 0.5040 - val_acc: 0.7693\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 352s - loss: 0.5214 - acc: 0.7830 - val_loss: 0.6575 - val_acc: 0.5895\n",
      "[loss , accuracy] con ReLu: [0.65747668426513672, 0.58948]\n",
      " \n",
      "Guardado como: k= 2 - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 351s - loss: 0.6559 - acc: 0.6716 - val_loss: 0.6365 - val_acc: 0.7888\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 352s - loss: 0.5378 - acc: 0.7478 - val_loss: 0.4803 - val_acc: 0.7876\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 361s - loss: 0.6836 - acc: 0.7008 - val_loss: 0.7805 - val_acc: 0.6017\n",
      "[loss , accuracy] con ReLu: [0.78052274192810056, 0.60167999999999999]\n",
      " \n",
      "Guardado como: k= 3 - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 355s - loss: 0.5986 - acc: 0.6941 - val_loss: 0.4588 - val_acc: 0.8429\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 351s - loss: 0.4820 - acc: 0.7870 - val_loss: 0.4474 - val_acc: 0.8424\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 351s - loss: 0.4957 - acc: 0.7908 - val_loss: 0.6911 - val_acc: 0.5958\n",
      "[loss , accuracy] con ReLu: [0.69106339843749998, 0.5958]\n",
      " \n",
      "Guardado como: k= 4 - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2\n",
      " \n",
      "Acc Media: 0.682248 Intervalo de Confianza: [0.532373168802 , 0.832122831198]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "np.random.seed(15)\n",
    "srng = RandomStreams(8)\n",
    "\n",
    "top_words = 3000\n",
    "embedding_vector_length = 32\n",
    "dropout = 0.5\n",
    "\n",
    "K = 5\n",
    "score = [None]*K\n",
    "\n",
    "for k in range(0,K):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "    score[k] = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print  \"[loss , accuracy] con ReLu: \" + str(score[k])\n",
    "    print \" \"\n",
    "    pred_x_train = np.c_[pred_x_train, model.predict(X_train)]\n",
    "    pred_x_test = np.c_[pred_x_test, model.predict(X_test)]\n",
    "    indice = 'k= ' + str(k) + ' - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2' \n",
    "    pred_indice = [pred_indice , indice]\n",
    "    print 'Guardado como: '+str(indice)\n",
    "    print ' '\n",
    "ci = mean_confidence_interval(np.transpose(score)[1])\n",
    "print 'Acc Media: ' + str(ci[0]) + ' Intervalo de Confianza: ['+ str(ci[1])+ ' , '+ str(ci[2])+ ']'\n",
    "print ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que, en este caso, cambiar la función de activación empeoró los resultados de los modelos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente experimento vuelve a la configuración inicial, con sigmoid, a lo que se agrega una capa más de profundidad, de 200 nodos. Esta capa adicional no es LSTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 361s - loss: 0.5565 - acc: 0.6936 - val_loss: 0.4002 - val_acc: 0.8205\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 361s - loss: 0.3413 - acc: 0.8608 - val_loss: 0.3249 - val_acc: 0.8682\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 357s - loss: 0.2845 - acc: 0.8861 - val_loss: 0.2973 - val_acc: 0.8735\n",
      "[loss , accuracy] con ReLu: [0.29734419645547866, 0.87348000000000003]\n",
      " \n",
      "Guardado como: k= 0 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.2 - deep2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 350s - loss: 0.4799 - acc: 0.7488 - val_loss: 0.3309 - val_acc: 0.8585\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 349s - loss: 0.3312 - acc: 0.8648 - val_loss: 0.3340 - val_acc: 0.8628\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 351s - loss: 0.2894 - acc: 0.8835 - val_loss: 0.3984 - val_acc: 0.8452\n",
      "[loss , accuracy] con ReLu: [0.39841155338287354, 0.84523999999999999]\n",
      " \n",
      "Guardado como: k= 1 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.2 - deep2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 350s - loss: 0.5059 - acc: 0.7290 - val_loss: 0.3943 - val_acc: 0.8308\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 350s - loss: 0.3197 - acc: 0.8694 - val_loss: 0.3267 - val_acc: 0.8634\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 349s - loss: 0.2747 - acc: 0.8904 - val_loss: 0.3970 - val_acc: 0.8580\n",
      "[loss , accuracy] con ReLu: [0.39696810646772385, 0.85799999999999998]\n",
      " \n",
      "Guardado como: k= 2 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.2 - deep2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 365s - loss: 0.4892 - acc: 0.7409 - val_loss: 0.3667 - val_acc: 0.8448\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 366s - loss: 0.3137 - acc: 0.8721 - val_loss: 0.3107 - val_acc: 0.8722\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 365s - loss: 0.2897 - acc: 0.8852 - val_loss: 0.3147 - val_acc: 0.8641\n",
      "[loss , accuracy] con ReLu: [0.31468937714576722, 0.86412]\n",
      " \n",
      "Guardado como: k= 3 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.2 - deep2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 366s - loss: 0.4664 - acc: 0.7618 - val_loss: 0.3897 - val_acc: 0.8390\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 365s - loss: 0.3208 - acc: 0.8676 - val_loss: 0.3069 - val_acc: 0.8716\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 366s - loss: 0.3150 - acc: 0.8773 - val_loss: 0.6398 - val_acc: 0.6045\n",
      "[loss , accuracy] con ReLu: [0.63984672870635984, 0.60451999999999995]\n",
      " \n",
      "Guardado como: k= 4 - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.2 - deep2\n",
      " \n",
      "Acc Media: 0.809072 Intervalo de Confianza: [0.666520716829 , 0.951623283171]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "np.random.seed(15)\n",
    "srng = RandomStreams(8)\n",
    "\n",
    "top_words = 3000\n",
    "embedding_vector_length = 32\n",
    "dropout = 0.5\n",
    "\n",
    "K = 5\n",
    "score = [None]*K\n",
    "\n",
    "for k in range(0,K):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(200, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "    score[k] = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print  \"[loss , accuracy] con ReLu: \" + str(score[k])\n",
    "    print \" \"\n",
    "    pred_x_train = np.c_[pred_x_train, model.predict(X_train)]\n",
    "    pred_x_test = np.c_[pred_x_test, model.predict(X_test)]\n",
    "    indice = 'k= ' + str(k) + ' - LSTM100 - top3000 - evl32 - sigmoid - binary - adam - dropout0.2 - deep2' \n",
    "    pred_indice = [pred_indice , indice]\n",
    "    print 'Guardado como: '+str(indice)\n",
    "    print ' '\n",
    "ci = mean_confidence_interval(np.transpose(score)[1])\n",
    "print 'Acc Media: ' + str(ci[0]) + ' Intervalo de Confianza: ['+ str(ci[1])+ ' , '+ str(ci[2])+ ']'\n",
    "print ' '\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que los resultados empeoraron al agregar una capa oculta. Observando las accuracy reportadas al final de la tercera epoch sobre el training set y sobre el test set, en la mayoría de los casos no se observa sobreajuste. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente grupo de experimentos se repite la arquitectura anterior, pero con función de ativación ReLu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 363s - loss: 0.6123 - acc: 0.6789 - val_loss: 1.1564 - val_acc: 0.7326\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 363s - loss: 0.5455 - acc: 0.7652 - val_loss: 0.4390 - val_acc: 0.8360\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 363s - loss: 1.3554 - acc: 0.6701 - val_loss: 7.9712 - val_acc: 0.0000e+00\n",
      "[loss , accuracy] con ReLu: [7.9711928564453123, 0.0]\n",
      " \n",
      "Guardado como: k= 0 - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2 - deep2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 364s - loss: 0.6024 - acc: 0.6898 - val_loss: 0.4380 - val_acc: 0.8146\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 348s - loss: 0.5561 - acc: 0.7522 - val_loss: 0.5883 - val_acc: 0.6519\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 348s - loss: 0.4868 - acc: 0.7737 - val_loss: 0.5280 - val_acc: 0.7578\n",
      "[loss , accuracy] con ReLu: [0.52799320792198179, 0.75783999999999996]\n",
      " \n",
      "Guardado como: k= 1 - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2 - deep2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 350s - loss: 0.6495 - acc: 0.6581 - val_loss: 0.5991 - val_acc: 0.6931\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 350s - loss: 0.5885 - acc: 0.7071 - val_loss: 0.8753 - val_acc: 0.6764\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 351s - loss: 0.5289 - acc: 0.7568 - val_loss: 0.5284 - val_acc: 0.7488\n",
      "[loss , accuracy] con ReLu: [0.52841274934768678, 0.74883999999999995]\n",
      " \n",
      "Guardado como: k= 2 - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2 - deep2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 349s - loss: 0.6140 - acc: 0.6848 - val_loss: 0.6078 - val_acc: 0.7952\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 349s - loss: 0.7574 - acc: 0.7520 - val_loss: 0.5059 - val_acc: 0.8124\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 350s - loss: 4.2937 - acc: 0.3990 - val_loss: 7.9712 - val_acc: 4.0000e-05\n",
      "[loss , accuracy] con ReLu: [7.9711998846435543, 4.0000000000000003e-05]\n",
      " \n",
      "Guardado como: k= 3 - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2 - deep2\n",
      " \n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 352s - loss: 0.6402 - acc: 0.6559 - val_loss: 0.5444 - val_acc: 0.7006\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 351s - loss: 0.4807 - acc: 0.7939 - val_loss: 0.8690 - val_acc: 0.7629\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 352s - loss: 0.5077 - acc: 0.7766 - val_loss: 0.5967 - val_acc: 0.6600\n",
      "[loss , accuracy] con ReLu: [0.59668733444213862, 0.66003999999999996]\n",
      " \n",
      "Guardado como: k= 4 - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2 - deep2\n",
      " \n",
      "Acc Media: 0.433352 Intervalo de Confianza: [-0.0601082351997 , 0.9268122352]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "np.random.seed(15)\n",
    "srng = RandomStreams(8)\n",
    "\n",
    "top_words = 3000\n",
    "embedding_vector_length = 32\n",
    "dropout = 0.5\n",
    "\n",
    "K = 5\n",
    "score = [None]*K\n",
    "\n",
    "for k in range(0,K):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "    score[k] = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print  \"[loss , accuracy] con ReLu: \" + str(score[k])\n",
    "    print \" \"\n",
    "    pred_x_train = np.c_[pred_x_train, model.predict(X_train)]\n",
    "    pred_x_test = np.c_[pred_x_test, model.predict(X_test)]\n",
    "    indice = 'k= ' + str(k) + ' - LSTM100 - top3000 - evl32 - ReLu - binary - adam - dropout0.2 - deep2' \n",
    "    pred_indice = [pred_indice , indice]\n",
    "    print 'Guardado como: '+str(indice)\n",
    "    print ' '\n",
    "ci = mean_confidence_interval(np.transpose(score)[1])\n",
    "print 'Acc Media: ' + str(ci[0]) + ' Intervalo de Confianza: ['+ str(ci[1])+ ' , '+ str(ci[2])+ ']'\n",
    "print ' '\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente el uso de ReLu produjo inestabilidades, con casos donde la exactitud cae a cero, lo que se traduce en un bajo desempeño promedio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para completar la tarea, se aprovecha a continuación todos los modelos construidos en los experimentos anteriores, en lo que se denomina un Stacking. En este caso se procederá simplemente a construir una regresión logística, a partir de la predicción levantada con cada uno de los modelos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(pred_x_train,y_train)\n",
    "#logistic.predict(pred_x_test,y_test)\n",
    "score = logistic.score(pred_x_test, y_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, se obtiene finalmente un modelo construido a partir de todos los modelos anteriores, que ofrece una accuracy de testing de 0.879, lo que resulta mejor que el promedio de cada configuración evaluada anteriormente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resumen\n",
    "\n",
    "<table>\n",
    "    <tr><td><strong>Modelo</strong></td><td><strong>Accuracy</strong></td></tr>\n",
    "    <tr><td>LSTM - Embedding = 32</td><td>0.87232</td></tr>\n",
    "    <tr><td>LSTM - Embedding = 8</td><td>0.853664</td></tr>\n",
    "    <tr><td>LSTM - Embedding = 32</td><td>0.867768</td></tr>\n",
    "    <tr><td>LSTM - Embedding = 64</td><td>0.858352</td></tr>\n",
    "    <tr><td>LSTM - Top Words = 1000</td><td>0.819272</td></tr>\n",
    "    <tr><td>LSTM - Top Words = 3000</td><td>0.85788</td></tr>\n",
    "    <tr><td>LSTM - Top Words = 6000</td><td>0.869304</td></tr>\n",
    "    <tr><td>LSTM - Dropout = 0.1</td><td>0.863</td></tr>\n",
    "    <tr><td>LSTM - Dropout = 0.2</td><td>0.807976</td></tr>\n",
    "    <tr><td>LSTM - Dropout = 0.5</td><td>0.863472</td></tr>\n",
    "    <tr><td>LSTM - ReLu</td><td>0.682248</td></tr>\n",
    "    <tr><td>LSTM + MLP 200 nodos</td><td>0.809072</td></tr>\n",
    "    <tr><td>LSTM + MLP 200 nodos - ReLu</td><td>0.433352</td></tr>\n",
    "    <tr><td><strong>Regresión Logística de predicciones (Stacking approach)</strong></td><td><strong>0.87888</strong></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones Generales\n",
    "\n",
    "En conclusión, este ejerecicio ha demostrado que utilizando deep learning con LSTM es posible construir modelos que predicen con una alta fiabilidad el sentimiento implicito en texto humano, en términos de positivo o negativo. Para ello es recomendable hacer uso de la mayor cantidad de palabras más frecuentes posible, lo que implica un mayor costo computacional, así como un vector de embedding suficientemente largo, pero evitando sobreajustar el modelo. No se pudo demostrar que Dropout genere un efecto positivo. Además, no se obtuvo buenos resultados en los modelos que utilizan ReLu en lugar de Sigmoid como función de activación. Finalmente, se constata que construir consorcios de modelo, por ejemplo mediante un stacking, es una buena forma de aprovechar lo mejor de cada configuración y conseguir un resultado más estable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Referencias\n",
    "* [1] Hastie, T.; Tibshirani, R., Friedman, J. (2009), The Elements of Statistical Learning, Second Edition. Springer New York Inc.\n",
    "* [2] Bishop, Christopher M. (1995), Neural Networks for Pattern Recognition, Clarendon Press.\n",
    "* [3] Graves, A. (2008), PhD Thesis. Supervised sequence labeling with recurrent neural networks. Technical University Munich.\n",
    "* [4] Box, G. E. P., Jenkins, G. M. and Reinsel, G. C. (1976), Time Series Analysis, Forecasting and Control. Third Edition. Holden-Day. Series G.\n",
    "* [5] Maas, A. L., Daly, R. E., Pham, P. T. and Huang, D., Ng, A. Y. and Potts, C. (2011), Learning Word Vectors for Sentiment Analysis, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1.\n",
    "* [6] https://www.kaggle.com/c/word2vec-nlp-tutorial/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
